# -*- coding: utf-8 -*-
"""中文分词软件使用.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GOqGdp8QePbRiC5iQphjkNOSS8hg26IB

### Describle

A Chinese word segmentation tutorial.

XU HAN

2020/09/29

Word segmentation tools include:
- jieba
- pyltp
- thulac
- nlpir
"""

import pandas as pd
df = pd.read_csv("./data/data.csv")
df.head()

"""### jieba

- https://github.com/fxsjy/jieba
"""

import jieba
def jieba_seg(s):
    segments = jieba.lcut(s) 
    s = " ".join(segments)
    return s

df["abs_jieba"] = df["abs"].apply(jieba_seg)
df["news_jieba"] = df["news"].apply(jieba_seg)
df["weibo_jieba"] = df["weibo"].apply(jieba_seg)

df[["abs_jieba", "news_jieba", "weibo_jieba"]]

"""### 哈工大pyltp
- https://pyltp.readthedocs.io/zh_CN/latest/
"""

import os
from pyltp import Segmentor
LTP_DATA_DIR = 'E:/ltp_data'  # ltp模型目录的路径
cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`


def ltp_seg(s):
    segmentor = Segmentor()  # 初始化实例
    segmentor.load(cws_model_path)  # 加载模型
    words = segmentor.segment(s)# 分词
    s = " ".join(words)
    segmentor.release()  # 释放模型
    return s

df["abs_ltp"] = df["abs"].apply(ltp_seg)
df["news_ltp"] = df["news"].apply(ltp_seg)
df["weibo_ltp"] = df["weibo"].apply(ltp_seg)

df[["abs_ltp", "news_ltp", "weibo_ltp"]]

"""###  清华THULAC

- https://github.com/thunlp/THULAC-Python
"""

import thulac	

thu1 = thulac.thulac(seg_only=True)  #默认模式
text = thu1.cut("我爱北京天安门", text=True)  #进行一句话分词
print(text)

def thu_seg(s):
    thu1 = thulac.thulac(seg_only=True) 
    s = thu1.cut(s, text=True)
    return s

df["abs_thu"] = df["abs"].apply(thu_seg)
df["news_thu"] = df["news"].apply(thu_seg)
df["weibo_thu"] = df["weibo"].apply(thu_seg)

df[["abs_thu", "news_thu", "weibo_thu"]]

"""###  中科院NLPIR

- https://github.com/tsroten/pynlpir
- https://pynlpir.readthedocs.io/en/latest/tutorial.html
- https://github.com/NLPIR-team/NLPIR
"""

import pynlpir
pynlpir.open()

s = '欢迎科研人员、技术工程师、企事业单位与个人参与NLPIR平台的建设工作。'
pynlpir.segment(s, pos_tagging=False)

def nlpir_seg(s):
    words = pynlpir.segment(s, pos_tagging=False)
    s = " ".join(words)
    return s

df["abs_nlpir"] = df["abs"].apply(nlpir_seg)
df["news_nlpir"] = df["news"].apply(nlpir_seg)
df["weibo_nlpir"] = df["weibo"].apply(nlpir_seg)

df[["abs_nlpir", "news_nlpir", "weibo_nlpir"]]

"""## 3. Result evaluation"""

def voting(jieba, ltp, thu, nlpir):
    '''
    对每个分词工具切分的词语在另外三种分词工具切分的词语列表中出现次数≥1的情况进行计数
    '''
    jlst = jieba.split()
    llst = ltp.split()
    tlst = thu.split()
    nlst = nlpir.split()
    
    count_j = 0
    count_l = 0
    count_t = 0
    count_n = 0
    
    for item in jlst:  
        if (item in llst) or (item in tlst) or (item in nlst):
            count_j += 1
            
    for item in llst:  
        if (item in jlst) or (item in tlst) or (item in nlst):
            count_l += 1    
            
    for item in tlst:  
        if (item in jlst) or (item in llst) or (item in nlst):
            count_t += 1            
            
    for item in nlst:  
        if (item in llst) or (item in tlst) or (item in jlst):
            count_n += 1      
            
    return count_j,count_l,count_t,count_n

#函数测试
voting(df["news_jieba"][0], df["news_ltp"][0], df["news_thu"][0], df["abs_nlpir"][0])

df["abs_eval"] = df.apply(lambda x: voting(x['abs_jieba'], x['abs_ltp'], x['abs_thu'],x['abs_nlpir']), axis=1)
df["news_eval"] = df.apply(lambda x: voting(x['news_jieba'], x['news_ltp'], x['news_thu'],x['news_nlpir']), axis=1)
df["weibo_eval"] = df.apply(lambda x: voting(x['weibo_jieba'], x['weibo_ltp'], x['weibo_thu'],x['weibo_nlpir']), axis=1)

# df.head()

# df.to_csv("./data/data_seg.csv", encoding="gb18030")

#对每一行实验语料的Voting计数进行求和
lst = list(df["weibo_eval"][0])
for i in range(1,df.shape[0]):
    tup = df["weibo_eval"][i]
    lst1 = list(tup)
    lst = [(lst[i]+lst1[i]) for i in range(0,len(lst))]

lst